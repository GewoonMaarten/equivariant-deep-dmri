{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the provided Layers and Network\n",
    "In this short tutorial we show how the proposed layers and network can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using and Configuring Layers\n",
    "Before we can use the layer, \n",
    "we need to define the types (channels and their orders) and q-space sampling schemas \n",
    "of the input and output feature maps.\n",
    "For the input, these are the same as the ones of the output feature map of the previous layer.\n",
    "In the first layer the q-space sampling schema is the one used in the dataset and the type is [1] (1 scalar channel)\n",
    "when (raw) dMRI scans as input.\n",
    "\n",
    "For the purpose of this example we will just hard-code these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# only 1 scalar channel\n",
    "type_in = [1]  \n",
    "\n",
    "# one scan with b=0 and the cubic sampling scheme (all 6 directions of the cube)\n",
    "q_sampling_schema_in = [[0., 0., 0.], \n",
    "                        [1., 0., 0.], [-1., 0., 0.], [0., 1., 0.], \n",
    "                        [0., -1., 0.], [0., 0., 1.], [0., 0., -1.]]\n",
    "\n",
    "# 2 scalar channels and 1 vector channel\n",
    "type_out = [2, 1]\n",
    "\n",
    "# we'll use the same sampling schema for the output, but we could instead also use a different one\n",
    "q_sampling_schema_out = q_sampling_schema_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pq-diff+p Layer\n",
    "Let's first define a layer using the pq-diff+p kernel, \n",
    "which is based on the pq-diff and the p-space kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: <EquivariantPQLayer (1,)->(2, 1)>\n",
      "Input:  <SphericalTensorType (1,)> with Q: 7\n",
      "Output:  <SphericalTensorType (2, 1)> with Q: 7\n",
      "Kernel: SumKernel(\n",
      "  (kernels): ModuleList(\n",
      "    (0): <Kernel_PQ (φ_cos(|p|) * φ_gauss(|q_out|) * φ_gauss(|q_in|)) * Y(p-q) of type (1,) -> (2, 1) with basis size (2, 1) * 200>\n",
      "    (1): <Kernel_PQ φ_cos(|p|) * Y(p) of type (1,) -> (2, 1) with basis size (2, 1) * 50>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from equideepdmri.layers.EquivariantPQLayer import EquivariantPQLayer\n",
    "\n",
    "layer = EquivariantPQLayer(type_in, type_out,\n",
    "                           kernel_definition=\"sum(pq_diff;p_space)\",\n",
    "                           p_kernel_size=5,\n",
    "                           q_sampling_schema_in=q_sampling_schema_in,\n",
    "                           q_sampling_schema_out=q_sampling_schema_out,\n",
    "                           p_radial_basis_type=\"cosine\",\n",
    "                           p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50})\n",
    "print('Layer:', layer)\n",
    "print('Input: ', layer.type_in, 'with Q:', layer.Q_in)\n",
    "print('Output: ', layer.type_out, 'with Q:', layer.Q_out)\n",
    "print('Kernel:', layer.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We defined to use the kernel size 5 in p-space.\n",
    "We also defined to use the cosine radial basis function\n",
    "with a 3 layer FC (and 50 units in each layer) applied to it for p-space.\n",
    "The default radial basis function would be the Gaussian without a FC applied to it,\n",
    "which is what is used for q-space as we did not define anything here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pq-diff+q Layer\n",
    "The definition of a layer using the pq-diff+q kernel is similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: <EquivariantPQLayer (1,)->(2, 1)>\n",
      "Input:  <SphericalTensorType (1,)> with Q: 7\n",
      "Output:  <SphericalTensorType (2, 1)> with Q: 7\n",
      "Kernel: SumKernel(\n",
      "  (kernels): ModuleList(\n",
      "    (0): <Kernel_PQ (φ_cos(|p|) * φ_gauss(|q_out|) * φ_gauss(|q_in|)) * Y(p-q) of type (1,) -> (2, 1) with basis size (2, 1) * 200>\n",
      "    (1): <Kernel_PQ (φ_gauss(|q_out|) * φ_gauss(|q_in|)) * Y(q) of type (1,) -> (2, 1) with basis size (2, 1) * 4>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layer = EquivariantPQLayer(type_in, type_out,\n",
    "                           kernel_definition=\"sum(pq_diff;q_space)\",\n",
    "                           p_kernel_size=5,\n",
    "                           q_sampling_schema_in=q_sampling_schema_in,\n",
    "                           q_sampling_schema_out=q_sampling_schema_out,\n",
    "                           p_radial_basis_type=\"cosine\",\n",
    "                           p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50})\n",
    "print('Layer:', layer)\n",
    "print('Input: ', layer.type_in, 'with Q:', layer.Q_in)\n",
    "print('Output: ', layer.type_out, 'with Q:', layer.Q_out)\n",
    "print('Kernel:', layer.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP-vec Layer\n",
    "To define a layer using the TP-vec kernel we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: <EquivariantPQLayer (1,)->(2, 1)>\n",
      "Input:  <SphericalTensorType (1,)> with Q: 7\n",
      "Output:  <SphericalTensorType (2, 1)> with Q: 7\n",
      "Kernel: <Kernel_PQ (φ_cos(|p|) * φ_gauss(|q_out|) * φ_gauss(|q_in|)) * (Y(q) x Y(p)) of type (1,) -> (2, 1) with basis size (2, 3) * 200>\n"
     ]
    }
   ],
   "source": [
    "layer = EquivariantPQLayer(type_in, type_out,\n",
    "                           kernel_definition=\"pq_TP\",\n",
    "                           p_kernel_size=5,\n",
    "                           q_sampling_schema_in=q_sampling_schema_in,\n",
    "                           q_sampling_schema_out=q_sampling_schema_out,\n",
    "                           p_radial_basis_type=\"cosine\",\n",
    "                           p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50},\n",
    "                           sub_kernel_selection_rule={0: [(0, 0)],\n",
    "                                                      1: [(0, 1), (1, 0), (1, 1)],\n",
    "                                                      2: [(2, 2)]})\n",
    "print('Layer:', layer)\n",
    "print('Input: ', layer.type_in, 'with Q:', layer.Q_in)\n",
    "print('Output: ', layer.type_out, 'with Q:', layer.Q_out)\n",
    "print('Kernel:', layer.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we can see that the used tuples $(l_\\mathrm{filter}, l_p, l_q)$ are defined\n",
    "in the `sub_kernel_selection_rule` parameter as a dict where the keys are the $l_\\mathrm{filter}$\n",
    "and the values are lists of pairs $(l_p, l_q)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP$\\pm$1 Layer\n",
    "To define a layer using the TP$\\pm$1 kernel we only adapt the `sub_kernel_selection_rule`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: <EquivariantPQLayer (1,)->(2, 1)>\n",
      "Input:  <SphericalTensorType (1,)> with Q: 7\n",
      "Output:  <SphericalTensorType (2, 1)> with Q: 7\n",
      "Kernel: <Kernel_PQ (φ_cos(|p|) * φ_gauss(|q_out|) * φ_gauss(|q_in|)) * (Y(q) x Y(p)) of type (1,) -> (2, 1) with basis size (4, 6) * 200>\n"
     ]
    }
   ],
   "source": [
    "layer = EquivariantPQLayer(type_in, type_out,\n",
    "                           kernel_definition=\"pq_TP\",\n",
    "                           p_kernel_size=5,\n",
    "                           q_sampling_schema_in=q_sampling_schema_in,\n",
    "                           q_sampling_schema_out=q_sampling_schema_out,\n",
    "                           p_radial_basis_type=\"cosine\",\n",
    "                           p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50},\n",
    "                           sub_kernel_selection_rule={\"l_diff_to_out_max\": 1})\n",
    "print('Layer:', layer)\n",
    "print('Input: ', layer.type_in, 'with Q:', layer.Q_in)\n",
    "print('Output: ', layer.type_out, 'with Q:', layer.Q_out)\n",
    "print('Kernel:', layer.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also remove the `sub_kernel_selection_rule` parameter as this value is the default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layers, q-Reduction, and Nonlinearities\n",
    "Now let's define multiple layers, add nonlinearities, q-reduction, and then p-space only layers.\n",
    "This is an architecture similar to the one used in the paper.\n",
    "\n",
    "First start with the pq-layers. \n",
    "There is a utility function that builds an `EquivariantPQLayer` together with a nonlinearity, \n",
    "it is called `build_pq_layer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv): <EquivariantPQLayer (1,)->(3, 1)>\n",
      "  (non_linearity): GatedBlockNonLin()\n",
      ")\n",
      "Input:  <SphericalTensorType (1,)> with Q: 7\n",
      "Output before nonlinearity:  <SphericalTensorType (3, 1)> with Q: 7\n",
      "Kernel: <Kernel_PQ (φ_cos(|p|) * φ_gauss(|q_out|) * φ_gauss(|q_in|)) * (Y(q) x Y(p)) of type (1,) -> (3, 1) with basis size (6, 6) * 200>\n"
     ]
    }
   ],
   "source": [
    "from equideepdmri.layers.layer_builders import build_pq_layer\n",
    "type_in = [1]\n",
    "type_out = [2, 1]\n",
    "\n",
    "pq_layer_1 = build_pq_layer(type_in, type_out,\n",
    "                            p_kernel_size=5,\n",
    "                            kernel=\"pq_TP\",\n",
    "                            q_sampling_schema_in=q_sampling_schema_in,\n",
    "                            q_sampling_schema_out=q_sampling_schema_out,\n",
    "                            p_radial_basis_type=\"cosine\",\n",
    "                            p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50},\n",
    "                            sub_kernel_selection_rule={\"l_diff_to_out_max\": 1},\n",
    "                            non_linearity_config={\"tensor_non_lin\":\"gated\", \"scalar_non_lin\":\"swish\"})\n",
    "print(pq_layer_1)\n",
    "print('Input: ', pq_layer_1[0].type_in, 'with Q:', pq_layer_1[0].Q_in)\n",
    "print('Output before nonlinearity: ', pq_layer_1[0].type_out, 'with Q:', pq_layer_1[0].Q_out)\n",
    "print('Kernel:', pq_layer_1[0].kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the used `non_linearity_config` is the default so it could also be omitted.\n",
    "The output before the nonlinearity has additional scalar channels (more than we defined), because these channels are needed for the gates in the non-linearity (one additional scalar channel for each non-scalar channel).\n",
    "\n",
    "Let's define the other pq-layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv): <EquivariantPQLayer (2, 1)->(6, 2, 1)>\n",
      "  (non_linearity): GatedBlockNonLin()\n",
      ")\n",
      "Input:  <SphericalTensorType (2, 1)> with Q: 7\n",
      "Output before nonlinearity:  <SphericalTensorType (6, 2, 1)> with Q: 7\n",
      "Kernel: <Kernel_PQ (φ_cos(|p|) * φ_gauss(|q_out|) * φ_gauss(|q_in|)) * (Y(q) x Y(p)) of type (2, 1) -> (6, 2, 1) with basis size (28, 78, 45, 9) * 200>\n"
     ]
    }
   ],
   "source": [
    "type_in = type_out  # output of previous layer is input to this one\n",
    "type_out = [3, 2, 1]\n",
    "pq_layer_2_type_out = type_out\n",
    "\n",
    "pq_layer_2 = build_pq_layer(type_in, type_out,\n",
    "                            p_kernel_size=5,\n",
    "                            kernel=\"pq_TP\",\n",
    "                            q_sampling_schema_in=q_sampling_schema_in,\n",
    "                            q_sampling_schema_out=q_sampling_schema_out,\n",
    "                            p_radial_basis_type=\"cosine\",\n",
    "                            p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50},\n",
    "                            sub_kernel_selection_rule={\"l_diff_to_out_max\": 1},\n",
    "                            non_linearity_config={\"tensor_non_lin\":\"gated\", \"scalar_non_lin\":\"swish\"})\n",
    "print(pq_layer_2)\n",
    "print('Input: ', pq_layer_2[0].type_in, 'with Q:', pq_layer_2[0].Q_in)\n",
    "print('Output before nonlinearity: ', pq_layer_2[0].type_out, 'with Q:', pq_layer_2[0].Q_out)\n",
    "print('Kernel:', pq_layer_2[0].kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now have non-scalar input and output channels, the kernel basis gets much larger and does not only have scalar and vector channels (as before) but also 45 l=2 and 9 l=3 channels (as can be seen in the basis size (29, 78, 45, 9).\n",
    "\n",
    "Now define the q-reduction. We'll use the `QLengthWeightedAvgPool` as used in the `late` approach.\n",
    "It can either be used by importing `QLengthWeightedAvgPool` from `layers.QLengthWeightedPool` or we can again use a layer builder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLengthWeightedAvgPool(\n",
      "  (radial_basis): FiniteElement_RadialBasis(\n",
      "    (model): FC()\n",
      "  )\n",
      ")\n",
      "<SphericalTensorType (3, 2, 1)>\n"
     ]
    }
   ],
   "source": [
    "from equideepdmri.layers.layer_builders import build_q_reduction_layer\n",
    "\n",
    "type_in = type_out\n",
    "q_reduction, type_out = build_q_reduction_layer(type_in, q_sampling_schema_in, reduction='length_weighted_average')\n",
    "\n",
    "print(q_reduction)\n",
    "print(q_reduction.type_in_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that besides `length_weighted_average` we could also use the unweighted `mean` or specify `conv` (as used in `gradual` q-reduction).\n",
    "\n",
    "Now (as q-space is reduced) let's define p-space layers. Note that no kernel needs to be specified as it is always `p_space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv): <EquivariantPLayer (3, 2, 1)->(2, 1)>\n",
      "  (non_linearity): GatedBlockNonLin()\n",
      ")\n",
      "Input:  <SphericalTensorType (3, 2, 1)> has Q: False\n",
      "Output before nonlinearity:  <SphericalTensorType (2, 1)> has Q: False\n",
      "Kernel: <Kernel_PQ φ_cos(|p|) * Y(p) of type (3, 2, 1) -> (2, 1) with basis size (8, 10, 5, 1) * 50> \n",
      "\n",
      "<EquivariantPLayer (1, 1)->(1,)>\n",
      "Input:  <SphericalTensorType (1, 1)> has Q: False\n",
      "Output before nonlinearity:  <SphericalTensorType (1,)> has Q: False\n",
      "Kernel: <Kernel_PQ φ_cos(|p|) * Y(p) of type (1, 1) -> (1,) with basis size (1, 1) * 50>\n"
     ]
    }
   ],
   "source": [
    "from equideepdmri.layers.layer_builders import build_p_layer\n",
    "\n",
    "type_out = [1, 1]\n",
    "p_layer_1 = build_p_layer(type_in, type_out,\n",
    "                          kernel_size=5,\n",
    "                          p_radial_basis_type=\"cosine\",\n",
    "                          p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50},\n",
    "                          non_linearity_config={\"tensor_non_lin\":\"gated\", \"scalar_non_lin\":\"swish\"})\n",
    "print(p_layer_1)\n",
    "print('Input: ', p_layer_1[0].type_in, 'has Q:', p_layer_1[0].has_Q_in)\n",
    "print('Output before nonlinearity: ', p_layer_1[0].type_out, 'has Q:', p_layer_1[0].has_Q_out)\n",
    "print('Kernel:', p_layer_1[0].kernel, '\\n')\n",
    "\n",
    "type_in = type_out\n",
    "type_out = [1]  # only 1 scalar channel as output\n",
    "\n",
    "# don't use nonlinearity as this is the last layer\n",
    "p_layer_2 = build_p_layer(type_in, type_out,\n",
    "                          kernel_size=5,\n",
    "                          p_radial_basis_type=\"cosine\",\n",
    "                          p_radial_basis_params={\"num_layers\": 3, \"num_units\": 50},\n",
    "                          use_non_linearity=False)\n",
    "print(p_layer_2) # no non-linearity => only EquivariantPLayer\n",
    "print('Input: ', p_layer_2.type_in, 'has Q:', p_layer_2.has_Q_in)\n",
    "print('Output before nonlinearity: ', p_layer_2.type_out, 'has Q:', p_layer_2.has_Q_out)\n",
    "print('Kernel:', p_layer_2.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Layers\n",
    "The layers can now be applied to some input feature map, where we'll use some random feature map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([1, 1, 7, 10, 10, 10])\n",
      "After pq-layer 1:  torch.Size([1, 5, 7, 10, 10, 10])\n",
      "After pq-layer 2:  torch.Size([1, 14, 7, 10, 10, 10])\n",
      "After q-reduction:  torch.Size([1, 14, 10, 10, 10])\n",
      "After p-layer 1:  torch.Size([1, 4, 10, 10, 10])\n",
      "After p-layer 2:  torch.Size([1, 1, 10, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 1, 7, 10, 10, 10)  # (batch_size x dim_in x Q_in x P_z x P_y x P_x)\n",
    "print(\"Input: \", x.size()) # Channel dim: 1*1 = 1\n",
    "\n",
    "x = pq_layer_1(x)\n",
    "print(\"After pq-layer 1: \", x.size()) # Channel dim: 2*1 + 1*3 = 5\n",
    "\n",
    "x = pq_layer_2(x)\n",
    "print(\"After pq-layer 2: \", x.size()) # Channel dim: 3*1 + 2*3 + 1*5 = 14\n",
    "\n",
    "x = q_reduction(x)\n",
    "print(\"After q-reduction: \", x.size()) # Channel dim unchanged (14), q-dim removed\n",
    "\n",
    "x = p_layer_1(x)\n",
    "print(\"After p-layer 1: \", x.size()) # Channel dim: 1*1 + 1*3 = 4\n",
    "\n",
    "x = p_layer_2(x)\n",
    "print(\"After p-layer 2: \", x.size()) # Channel dim: 1*1 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the provided Voxel-Wise Segmentation Network\n",
    "As shown before, the provided equivariant layers can be stacked to build equivariant networks.\n",
    "For voxel-wise prediction (e.g. voxel-wise segmentation) we included a network.\n",
    "This network uses the architecture described in the paper where first pq-layers are applied, then a q-reduction, and then p-layers. This is the same structure as we defined previusly in this example with the layer builders.\n",
    "\n",
    "In the following sections we show how the segmentation network might be used an trained.\n",
    "\n",
    "### Preparation of the Dataset\n",
    "For the purpose of this example we will use a randomly generated dataset.\n",
    "This means that real learning might not be possible but it still shows how the segmentation network could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from example.utils import RandomDMriSegmentationDataset\n",
    "dataset = RandomDMriSegmentationDataset(N=10, Q=8, num_b0=2, p_size=(10, 10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `RandomDMriSegmentationDataset` contains samples with the same p-size.\n",
    "This is just for simplicity of this example, in practice the `VoxelWiseSegmentationNetwork` can handle different sizes of the samples (as it is fully-convolutional). In our training we for example cropped all scans to the bounding boxes of their brain masks to save memory and speed up the training.\n",
    "\n",
    "### Defining the Network\n",
    "Now we define the network. The hyperparameters are the same as the ones used in our best model shown in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoxelWiseSegmentationNetwork(\n",
      "  (pq_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (conv): <EquivariantPQLayer (1,)->(11, 4)>\n",
      "      (non_linearity): GatedBlockNonLin()\n",
      "    )\n",
      "  )\n",
      "  (q_reduction_layer): QLengthWeightedAvgPool(\n",
      "    (radial_basis): FiniteElement_RadialBasis(\n",
      "      (model): FC()\n",
      "    )\n",
      "  )\n",
      "  (p_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (conv): <EquivariantPLayer (7, 4)->(25, 5)>\n",
      "      (non_linearity): GatedBlockNonLin()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (conv): <EquivariantPLayer (20, 5)->(13, 3)>\n",
      "      (non_linearity): GatedBlockNonLin()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (conv): <EquivariantPLayer (10, 3)->(7, 2)>\n",
      "      (non_linearity): GatedBlockNonLin()\n",
      "    )\n",
      "    (3): <EquivariantPLayer (5, 2)->(1,)>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from equideepdmri.network.VoxelWiseSegmentationNetwork import VoxelWiseSegmentationNetwork\n",
    "model = VoxelWiseSegmentationNetwork(\n",
    "    q_sampling_schema_in=dataset.q_sampling_schema,\n",
    "    pq_channels=[\n",
    "        [7, 4]\n",
    "    ],\n",
    "    p_channels=[\n",
    "        [20, 5],\n",
    "        [10, 3],\n",
    "        [5, 2],\n",
    "        [1]\n",
    "    ],\n",
    "    pq_kernel={\n",
    "        'kernel':'pq_TP',\n",
    "        'p_radial_basis_type':'cosine'\n",
    "    },\n",
    "    p_kernel={\n",
    "        'p_radial_basis_type':'cosine'\n",
    "    },\n",
    "    kernel_sizes=5,\n",
    "    non_linearity={\n",
    "        'tensor_non_lin':'gated',\n",
    "        'scalar_non_lin':'swish'\n",
    "    },\n",
    "    q_reduction={\n",
    "        'reduction':'length_weighted_average'\n",
    "    }\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network\n",
    "Now we train the network using our random dataset. (This example may never converge as the data is random).\n",
    "The following code is a simplified version of the training code we used for our paper, e.g. validation (and computing metrics), logging and saving predicted samples, saving checkpoints, and early stopping were removed for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 14.580911636352539\n",
      "Loss: 13.333956718444824\n",
      "Loss: 10.981715202331543\n",
      "Loss: 9.749805450439453\n",
      "Loss: 7.926243305206299\n",
      "Loss: 6.447773456573486\n",
      "Loss: 5.228459358215332\n",
      "Loss: 4.740058422088623\n",
      "Loss: 4.1125640869140625\n",
      "Loss: 3.674058675765991\n",
      "Loss: 3.2416303157806396\n",
      "Loss: 2.5309395790100098\n",
      "Loss: 2.121861219406128\n",
      "Loss: 1.8830928802490234\n",
      "Loss: 1.4352169036865234\n",
      "Loss: 1.174275279045105\n",
      "Loss: 0.8579174876213074\n",
      "Loss: 0.7635501027107239\n",
      "Loss: 0.64266437292099\n",
      "Loss: 0.5872637033462524\n",
      "Loss: 0.5563927888870239\n",
      "Loss: 0.5204508900642395\n",
      "Loss: 0.5092914700508118\n",
      "Loss: 0.5066750645637512\n",
      "Loss: 0.5145171880722046\n",
      "Loss: 0.47252941131591797\n",
      "Loss: 0.48094403743743896\n",
      "Loss: 0.4816356301307678\n",
      "Loss: 0.4658873379230499\n",
      "Loss: 0.45935654640197754\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from example.utils import compute_binary_label_weights\n",
    "\n",
    "epochs = 3\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "pos_weight = compute_binary_label_weights(dataloader)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5.0e-03)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in iter(dataloader):\n",
    "        sample_ids, x, target, brain_mask = batch['sample_id'], batch['input'], batch['target'], batch['brain_mask']\n",
    "\n",
    "        assert brain_mask.size(0) == 1 and len(sample_ids) == 1 and target.size(0) == 1 and x.size(0) == 1, \\\n",
    "                        'Currently only batch-size 1 is supported'\n",
    "        sample_ids = sample_ids[0]\n",
    "        brain_mask = brain_mask.squeeze(0).bool()  # (Z x Y x X)\n",
    "        target = target.squeeze(0)[brain_mask]  # (num_non_masked_voxels)\n",
    "        # note: x is not squeezed as model expected batch dim, it is squeezed after model is applied\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted_scores = model(x).squeeze(0)  # (Z x Y x X)\n",
    "        predicted_scores = predicted_scores[brain_mask]  # (num_non_masked_voxels)\n",
    "        loss = criterion(predicted_scores, target)\n",
    "        print('Loss:', float(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
